{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BEDBASE Demo**\n",
    "\n",
    "The following demo has the purpose of demonstrating how to process, generate statistics and plots of BED files genrated by the R package Genomic Distributions using the REST API for the bedstat and bedbuncher pipelines. \n",
    "\n",
    "The general workflow for uploading bed files and their \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior to start the tutorial (files download)\n",
    "We need create a directory where we'll store the bedbase pipelines and files to be processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $HOME/Desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir bedbase_tutorial\n",
    "cd bedbase_tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download the BED files and PEPs we'll need for this demo, we can easily do this with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-03-17 16:20:59--  http://big.databio.org/example_data/bedbase_demo/bedbase_demo_files_justBED/bedbase_BEDfiles.tar.gz\n",
      "Resolving big.databio.org (big.databio.org)... 128.143.245.181\n",
      "Connecting to big.databio.org (big.databio.org)|128.143.245.181|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 60245813 (57M) [application/octet-stream]\n",
      "Saving to: ‘bedbase_BEDfiles.tar.gz’\n",
      "\n",
      "bedbase_BEDfiles.ta 100%[===================>]  57.45M   102MB/s    in 0.6s    \n",
      "\n",
      "2020-03-17 16:20:59 (102 MB/s) - ‘bedbase_BEDfiles.tar.gz’ saved [60245813/60245813]\n",
      "\n",
      "--2020-03-17 16:21:00--  http://big.databio.org/example_data/bedbase_demo/bedbase_demo_files_justBED/bedbase_demo_PEPs.tar.gz\n",
      "Resolving big.databio.org (big.databio.org)... 128.143.245.181\n",
      "Connecting to big.databio.org (big.databio.org)|128.143.245.181|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1262 (1.2K) [application/octet-stream]\n",
      "Saving to: ‘bedbase_demo_PEPs.tar.gz’\n",
      "\n",
      "bedbase_demo_PEPs.t 100%[===================>]   1.23K  --.-KB/s    in 0s      \n",
      "\n",
      "2020-03-17 16:21:00 (220 MB/s) - ‘bedbase_demo_PEPs.tar.gz’ saved [1262/1262]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wget http://big.databio.org/example_data/bedbase_demo/bedbase_demo_files_justBED/bedbase_BEDfiles.tar.gz     \n",
    "wget http://big.databio.org/example_data/bedbase_demo/bedbase_demo_files_justBED/bedbase_demo_PEPs.tar.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use our files and PEPs, we need to untar them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bedbase_BEDfiles/\n",
      "bedbase_BEDfiles/GSE105977_ENCFF449EZT_optimal_idr_thresholded_peaks_hg19.bed.gz\n",
      "bedbase_BEDfiles/GSE105587_ENCFF018NNF_conservative_idr_thresholded_peaks_GRCh38.bed.gz\n",
      "bedbase_BEDfiles/GSE105587_ENCFF413ANK_peaks_hg19.bed.gz\n",
      "bedbase_BEDfiles/GSM2423312_ENCFF155HVK_peaks_GRCh38.bed.gz\n",
      "bedbase_BEDfiles/GSE105977_ENCFF617QGK_optimal_idr_thresholded_peaks_GRCh38.bed.gz\n",
      "bedbase_BEDfiles/GSE91663_ENCFF316ASR_peaks_GRCh38.bed.gz\n",
      "bedbase_BEDfiles/GSM2423313_ENCFF722AOG_peaks_GRCh38.bed.gz\n",
      "bedbase_BEDfiles/GSE105587_ENCFF809OOE_conservative_idr_thresholded_peaks_hg19.bed.gz\n",
      "bedbase_BEDfiles/GSM2827349_ENCFF196DNQ_peaks_GRCh38.bed.gz\n",
      "bedbase_BEDfiles/GSE91663_ENCFF553KIK_optimal_idr_thresholded_peaks_GRCh38.bed.gz\n",
      "bedbase_BEDfiles/GSE91663_ENCFF319TPR_conservative_idr_thresholded_peaks_GRCh38.bed.gz\n",
      "bedbase_BEDfiles/GSE105977_ENCFF634NTU_peaks_hg19.bed.gz\n",
      "bedbase_BEDfiles/GSE105977_ENCFF937CGY_peaks_GRCh38.bed.gz\n",
      "bedbase_BEDfiles/GSM2827350_ENCFF928JXU_peaks_GRCh38.bed.gz\n",
      "bedbase_BEDfiles/GSE105977_ENCFF793SZW_conservative_idr_thresholded_peaks_GRCh38.bed.gz\n",
      "bedbase_demo_PEPs/\n",
      "bedbase_demo_PEPs/bedstat_annotation_sheet.csv\n",
      "bedbase_demo_PEPs/bedbuncher_config.yaml\n",
      "bedbase_demo_PEPs/bedbuncher_query.csv\n",
      "bedbase_demo_PEPs/bedstat_config.yaml\n"
     ]
    }
   ],
   "source": [
    "tar -zxvf bedbase_BEDfiles.tar.gz\n",
    "tar -zxvf bedbase_demo_PEPs.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First part of the tutorial (insert BED files stats into elastic)\n",
    "\n",
    "\n",
    "### 1) Create a PEP describing the BED files to process\n",
    "\n",
    "In order to get started, we'll need a PEP [Portable Encapsulated project](https://pepkit.github.io/). A PEP consists of 1) an annotation sheet (.csv) that contains information about the samples on a project and 2) a project config.yaml file that points to the sample annotation sheet. THe config file also has other components, such as derived attributes, that in this case point to the BED files to be processed. The following is an example of a config file using the derived attributes output_file_path and yaml_file to point to the `.bed.gz` files and their respective metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata:\n",
      "  sample_table: bedstat_annotation_sheet.csv\n",
      "  output_dir: ../bedstat/bedstat_pipeline_logs \n",
      "  pipeline_interfaces: ../bedstat/pipeline_interface.yaml\n",
      "\n",
      "constant_attributes: \n",
      "  output_file_path: \"source\"\n",
      "  yaml_file: \"source2\"\n",
      "  protocol: \"bedstat\"\n",
      "\n",
      "derived_attributes: [output_file_path, yaml_file]\n",
      "data_sources:\n",
      "  source: \"../bedbase_BEDfiles/{file_name}\" \n",
      "  source2: \"../bedstat/bedstat_pipeline_logs/submission/{sample_name}.yaml\"\n"
     ]
    }
   ],
   "source": [
    "cat bedbase_demo_PEPs/bedstat_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Download bedstat and the Bedbase configuration manager (bbconf)\n",
    "\n",
    "[bedstat](https://github.com/databio/bedstat) is a [pypiper](http://code.databio.org/pypiper/) pipeline that generates statistics and plots of BED files. [bbconf](https://github.com/databio/bbconf) implements convenience methods for interacting with the database backend, which in this case is defined by an Elastic search local cluster. For carrying out this demo, we'll be using the dev version of `bbconf` that can be download as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'bedstat'...\n",
      "remote: Enumerating objects: 161, done.\u001b[K\n",
      "remote: Counting objects: 100% (161/161), done.\u001b[K\n",
      "remote: Compressing objects: 100% (88/88), done.\u001b[K\n",
      "Receiving objects: 100% (358/358), 57.22 KiB | 5.72 MiB/s, done.\n",
      "remote: Total 358 (delta 78), reused 109 (delta 43), pack-reused 197\u001b[K\n",
      "Resolving deltas: 100% (152/152), done.\n",
      "Collecting git+https://github.com/databio/bbconf.git@dev\n",
      "  Cloning https://github.com/databio/bbconf.git (to revision dev) to /tmp/pip-req-build-evqcaq50\n",
      "  Running command git clone -q https://github.com/databio/bbconf.git /tmp/pip-req-build-evqcaq50\n",
      "  Running command git checkout -b dev --track origin/dev\n",
      "  Switched to a new branch 'dev'\n",
      "  Branch 'dev' set up to track remote branch 'dev' from 'origin'.\n",
      "Requirement already satisfied (use --upgrade to upgrade): bbconf==0.0.2.dev0 from git+https://github.com/databio/bbconf.git@dev in /home/jev4xy/.local/lib/python3.6/site-packages\n",
      "Requirement already satisfied: elasticsearch in /home/jev4xy/.local/lib/python3.6/site-packages (from bbconf==0.0.2.dev0) (7.1.0)\n",
      "Requirement already satisfied: logmuse in /home/jev4xy/.local/lib/python3.6/site-packages (from bbconf==0.0.2.dev0) (0.2.5)\n",
      "Requirement already satisfied: yacman>=0.6.6 in /home/jev4xy/.local/lib/python3.6/site-packages (from bbconf==0.0.2.dev0) (0.6.6)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/lib/python3/dist-packages (from elasticsearch->bbconf==0.0.2.dev0) (1.22)\n",
      "Requirement already satisfied: oyaml in /home/jev4xy/.local/lib/python3.6/site-packages (from yacman>=0.6.6->bbconf==0.0.2.dev0) (0.9)\n",
      "Requirement already satisfied: pyyaml>=3.13 in /home/jev4xy/.local/lib/python3.6/site-packages (from yacman>=0.6.6->bbconf==0.0.2.dev0) (5.1.2)\n",
      "Requirement already satisfied: ubiquerg>=0.4.9 in /home/jev4xy/.local/lib/python3.6/site-packages (from yacman>=0.6.6->bbconf==0.0.2.dev0) (0.5.0)\n",
      "Requirement already satisfied: attmap>=0.12.9 in /home/jev4xy/.local/lib/python3.6/site-packages (from yacman>=0.6.6->bbconf==0.0.2.dev0) (0.12.9)\n",
      "Building wheels for collected packages: bbconf\n",
      "  Building wheel for bbconf (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bbconf: filename=bbconf-0.0.2.dev0-cp36-none-any.whl size=8961 sha256=c063f39d13c3cca85cbc49fc1f12da2ca1e9741a4d7cf59fd05781b201cd087c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-dq_ib5w4/wheels/86/ec/ae/8d3556156f53eca4b9c93e66c52a7f789ff9deb2b5a9c0663e\n",
      "Successfully built bbconf\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "git clone git@github.com:databio/bedstat\n",
    "# Install Python dependencies\n",
    "pip install piper --user\n",
    "pip install --user loopercli\n",
    "pip install git+https://github.com/databio/bbconf.git@dev\n",
    "# Install R dependencies\n",
    "Rscript scripts/installRdeps.R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to create a directory where we can store the stats and plots generated by `bedstat`. Additionally, we'll create a directory where we can store log and metadata files that we'll need later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir bedstat/bedstat_output\n",
    "mkdir bedstat/bedstat_pipeline_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use bbconf, we'll need to create a minimal configuration.yaml file. The path to this configuration file can be stored in the environment variable `$BEDBASE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path:\n",
      "  pipelines_output: $LABROOT/resources/regions/bedstat_output\n",
      "\n",
      "database:\n",
      "  host: localhost\n",
      "  bed_index: bed_index\n",
      "  bedset_index: bedset_index\n",
      "\n",
      "server:\n",
      "  host: 0.0.0.0\n",
      "  port: 8000\n"
     ]
    }
   ],
   "source": [
    "cat bedbase_demoPEPs/bedbase_configuration.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Inititiate a local elasticsearch cluster\n",
    "\n",
    "In addition to generate statistics and plots, [bedstat](https://github.com/databio/bedstat) inserts JSON formatted metadata into an [elasticsearch](https://www.elastic.co/elasticsearch/?ultron=[EL]-[B]-[AMER]-US+CA-Exact&blade=adwords-s&Device=c&thor=elasticsearch&gclid=Cj0KCQjwjcfzBRCHARIsAO-1_Oq5mSdze16kripxT5_I__EeH9F-xUCz_khEvzGL7q_mqP62CahJ9SIaAg2BEALw_wcB) database that it'll later be used to search and extract files and information about them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If docker is not already installed, you can do so with the following commands\n",
    "#(make sure you have sudo permissions)\n",
    "sudo apt-get update\n",
    "sudo apt-get install docker-engine -y\n",
    "\n",
    "# Create a persistent volume to house elastic search data\n",
    "docker volume create es-data\n",
    "\n",
    "# Run the docker container for elasticsearch\n",
    "docker run -p 9200:9200 -p 9300:9300 -v es-data:/usr/share/elasticsearch/data -e \"xpack.ml.enabled=false\" \\\n",
    "  -e \"discovery.type=single-node\" elasticsearch:7.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Run the bedstat pipeline on the demo PEP\n",
    "To run [bedstat](https://github.com/databio/bedstat) and the other required pipelines in this demo, we will rely on the pipeline submission engine [looper](http://looper.databio.org/en/latest/). For detailed instructions in how to link a project to a pipeline, click [here](http://looper.databio.org/en/latest/linking-a-pipeline/). If the pipeline is being run from an HPC environment where docker is not available, we recommend running the pipeline using the `--no-db-commit` flag (this will only calculate statistics and generate plots but will not insert this information into the local elasticsearch cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command: run (Looper version: 0.12.4)\n",
      "Reading sample table: '/home/jev4xy/Desktop/bedbase_tutorial/bedbase_demo_PEPs/bedstat_annotation_sheet.csv'\n",
      "Activating compute package 'local'\n",
      "Finding pipelines for protocol(s): bedstat\n",
      "Known protocols: bedstat\n",
      "'/home/jev4xy/Desktop/bedbase_tutorial/bedbase_demo_PEPs/../bedstat/pipeline/bedstat.py' appears to attempt to run on import; does it lack a conditional on '__main__'? Using base type: Sample\n",
      "\u001b[36m## [1 of 15] bedhost_demo_db1 (bedstat)\u001b[0m\n",
      "Submission settings lack memory specification\n",
      "Writing script to /home/jev4xy/Desktop/bedstat/bedstat_pipeline_logs/submission/bedstat_bedhost_demo_db1.sub\n",
      "Job script (n=1; 0.00 Gb): ../bedstat/bedstat_pipeline_logs/submission/bedstat_bedhost_demo_db1.sub\n",
      "Compute node: cphg-51ksmr2\n",
      "Start time: 2020-03-18 12:39:50\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jev4xy/Desktop/bedbase_tutorial/bedbase_demo_PEPs/../bedstat/pipeline/bedstat.py\", line 38, in <module>\n",
      "    bed_digest = md5(open(args.bedfile, 'rb').read()).hexdigest()\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../bedbase_BEDfiles/GSE105587_ENCFF018NNF_conservative_idr_thresholded_peaks_GRCh38.bed.gz'\n",
      "\n",
      "Looper finished\n",
      "Samples valid for job generation: 1 of 1\n",
      "Successful samples: 1 of 1\n",
      "Commands submitted: 1 of 1\n",
      "Jobs submitted: 1\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "looper run bedbase_demo_PEPs/bedstat_config.yaml --no-db-commit --compute local --limit 1 -R\n",
    "\n",
    "#looper run bedbase_demo_PEPs/bedstat_config.yaml --bedbase-config bedbase_demo_PEPs/bedbase_configuration.yaml \\\n",
    "#--no-db-commit --compute local -R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have generated plots and statistics, we can insert them into our local elastic search cluster running the bedstat pipeline with the `--just-db-commit` flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "looper run bedbase_demo_PEPs/bedstat_config.yaml  --just-db-commit --compute local -R\n",
    "\n",
    "#looper run bedbase_demo_PEPs/bedstat_config.yaml --bedbase-config bedbase_demo_PEPs/bedbase_configuration.yaml \\\n",
    "#--just-db-commit --compute local -R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the previous steps have been executed, our BED files should be available for query on our local elastic search cluster. Files can be queried using the `bedbuncher` pipeline described in the below section. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second part of the tutorial (use bedbuncher to create bedsets)\n",
    "\n",
    "### 1) Create a new PEP describing the bedset name and specific JSON query  \n",
    "[bedbuncher](https://github.com/databio/bedbuncher) is a pipeline designed to create bedsets (sets of BED files retrieved from bedbase). In order to create bedsets, we will need to create an additional PEP describing the query as well as attributes such as the name assigned to the newly created bedset. This configuration file should descibe the path to the `JSON` query file. THe configuration file should have the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/Desktop/bedbuncher/project\n",
    "cat bedset_query.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/Desktop/bedbuncher/project\n",
    "cat cfg.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Run the bedbuncher pipeline with looper\n",
    "\n",
    "In order to create a bedset, we simply need to create a PEP as previously shown and run the bedbuncher pipeline using looper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/Desktop/bedbuncher\n",
    "looper run project/cfg.yaml --compute local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third part of the demo (run local instance of bedhost)\n",
    "\n",
    "The last part of the tutorial consists on running a local instance of [bedhost](https://github.com/databio/bedhost/tree/master) (a REST API for bedstat and bedbuncher produced outputs) in order to explore and download output files. To access the API, we'll need to download the dev branch of the github repository as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git clone git@github.com:databio/bedhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to run the following command, making sure to point to the previously described bedbase config.yaml file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedhost serve -c path/to/config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have stored the path to the bedbase config in the environment variable `$BEDBASE` (suggested), it's not neccesary to specify the path to the config file to start bedhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedhost serve "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
